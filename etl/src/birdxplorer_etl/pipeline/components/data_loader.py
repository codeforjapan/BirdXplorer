"""
DataLoaderComponent for loading transformed data to S3.

This component wraps the existing S3 upload logic
from load.py into a reusable pipeline component.
"""

import logging
import os
from datetime import datetime
from typing import Any, Dict, List, Optional

import boto3
from botocore.exceptions import ClientError, NoCredentialsError

from birdxplorer_etl.pipeline.base.component import PipelineComponent, PipelineComponentError
from birdxplorer_etl.pipeline.base.context import PipelineContext
from birdxplorer_etl.settings import S3_BUCKET_NAME


class DataLoaderComponent(PipelineComponent):
    """
    Pipeline component for loading transformed data to S3.
    
    This component uploads CSV files generated by the transformation
    step to an S3 bucket with organized directory structure.
    """

    def __init__(self, name: str, config: Optional[Dict[str, Any]] = None) -> None:
        """
        Initialize the DataLoaderComponent.

        Args:
            name: Unique name for this component instance
            config: Optional configuration dictionary
        """
        super().__init__(name, config)
        self.logger = logging.getLogger(__name__)
        self.s3_client = None

    def validate_config(self) -> None:
        """Validate the component's configuration."""
        bucket_name = self.get_config_value("s3_bucket_name", S3_BUCKET_NAME)
        if not bucket_name:
            raise ValueError("S3 bucket name must be configured")
            
        region = self.get_config_value("aws_region", "ap-northeast-1")
        if not isinstance(region, str):
            raise ValueError("AWS region must be a string")
            
        input_dir = self.get_config_value("input_directory", "./data/transformed")
        if not isinstance(input_dir, str):
            raise ValueError("input_directory must be a string")

    def setup(self, context: PipelineContext) -> None:
        """Setup S3 client before execution."""
        try:
            region = self.get_config_value("aws_region", "ap-northeast-1")
            self.s3_client = boto3.client("s3", region_name=region)
            self.logger.info(f"S3 client initialized for region: {region}")
        except Exception as e:
            raise PipelineComponentError(self, f"Failed to initialize S3 client: {e}", e)

    def execute(self, context: PipelineContext) -> PipelineContext:
        """
        Execute the data loading to S3.

        Args:
            context: The pipeline context

        Returns:
            The modified pipeline context

        Raises:
            PipelineComponentError: If loading fails
        """
        try:
            self.logger.info(f"Starting data loading to S3 with component: {self.name}")
            
            # Get configuration
            bucket_name = self.get_config_value("s3_bucket_name", S3_BUCKET_NAME)
            input_dir = self.get_config_value("input_directory", "./data/transformed")
            
            if not bucket_name:
                self.logger.warning("S3 bucket name not configured, skipping upload")
                context.set_metadata(f"{self.name}_status", "skipped")
                context.set_metadata(f"{self.name}_reason", "No S3 bucket configured")
                return context
            
            # Generate object prefix with timestamp
            object_prefix = self._generate_object_prefix()
            
            # Get list of files to upload
            files_to_upload = self._get_files_to_upload(input_dir)
            
            if not files_to_upload:
                self.logger.warning(f"No files found in {input_dir}")
                context.set_metadata(f"{self.name}_status", "completed")
                context.set_metadata(f"{self.name}_files_uploaded", 0)
                return context
            
            # Upload files to S3
            uploaded_files = self._upload_files_to_s3(files_to_upload, bucket_name, object_prefix)
            
            self.logger.info(f"Data loading completed. Uploaded {len(uploaded_files)} files to S3")
            
            # Update context with loading results
            context.set_metadata(f"{self.name}_status", "completed")
            context.set_metadata(f"{self.name}_files_uploaded", len(uploaded_files))
            context.set_metadata(f"{self.name}_s3_bucket", bucket_name)
            context.set_metadata(f"{self.name}_s3_prefix", object_prefix)
            context.set_metadata(f"{self.name}_uploaded_files", uploaded_files)
            
            return context
            
        except Exception as e:
            self.logger.error(f"Data loading failed: {e}")
            raise PipelineComponentError(self, f"Loading failed: {e}", e)

    def _generate_object_prefix(self) -> str:
        """
        Generate S3 object prefix with timestamp.
        
        Returns:
            S3 object prefix string
        """
        timestamp_format = self.get_config_value("timestamp_format", "%Y-%m-%d %H:%M")
        current_time = datetime.now().strftime(timestamp_format)
        return f"{current_time}/"

    def _get_files_to_upload(self, input_dir: str) -> List[str]:
        """
        Get list of files to upload from input directory.
        
        Args:
            input_dir: Input directory path
            
        Returns:
            List of file paths to upload
        """
        # Default file list from the original implementation
        default_files = [
            "media.csv",
            "note_topic_association.csv", 
            "note.csv",
            "post_link_association.csv",
            "post_link.csv",
            "post_media_association.csv",
            "post.csv",
            "topic.csv",
            "user.csv",
        ]
        
        # Get file list from config or use default
        file_patterns = self.get_config_value("file_patterns", default_files)
        
        files_to_upload = []
        for pattern in file_patterns:
            file_path = os.path.join(input_dir, pattern)
            if os.path.exists(file_path):
                files_to_upload.append(file_path)
            else:
                self.logger.warning(f"File not found: {file_path}")
                
        return files_to_upload

    def _upload_files_to_s3(self, files: List[str], bucket_name: str, object_prefix: str) -> List[Dict[str, str]]:
        """
        Upload files to S3 bucket.
        
        Args:
            files: List of file paths to upload
            bucket_name: S3 bucket name
            object_prefix: S3 object prefix
            
        Returns:
            List of upload results with file info
        """
        uploaded_files = []
        
        for file_path in files:
            try:
                # Extract filename from path
                filename = os.path.basename(file_path)
                s3_key = f"{object_prefix}{filename}"
                
                # Upload file to S3
                self.s3_client.upload_file(file_path, bucket_name, s3_key)
                
                upload_info = {
                    "local_path": file_path,
                    "s3_bucket": bucket_name,
                    "s3_key": s3_key,
                    "file_size": os.path.getsize(file_path),
                    "status": "success"
                }
                uploaded_files.append(upload_info)
                
                self.logger.info(f"Successfully uploaded {filename} to S3")
                
            except FileNotFoundError:
                error_info = {
                    "local_path": file_path,
                    "s3_bucket": bucket_name,
                    "s3_key": f"{object_prefix}{os.path.basename(file_path)}",
                    "status": "error",
                    "error": "File not found"
                }
                uploaded_files.append(error_info)
                self.logger.error(f"File not found: {file_path}")
                
            except NoCredentialsError:
                error_info = {
                    "local_path": file_path,
                    "s3_bucket": bucket_name,
                    "s3_key": f"{object_prefix}{os.path.basename(file_path)}",
                    "status": "error",
                    "error": "AWS credentials not found"
                }
                uploaded_files.append(error_info)
                self.logger.error("AWS credentials not found")
                
            except ClientError as e:
                error_code = e.response['Error']['Code']
                error_message = e.response['Error']['Message']
                error_info = {
                    "local_path": file_path,
                    "s3_bucket": bucket_name,
                    "s3_key": f"{object_prefix}{os.path.basename(file_path)}",
                    "status": "error",
                    "error": f"AWS error {error_code}: {error_message}"
                }
                uploaded_files.append(error_info)
                self.logger.error(f"AWS error uploading {file_path}: {error_code} - {error_message}")
                
            except Exception as e:
                error_info = {
                    "local_path": file_path,
                    "s3_bucket": bucket_name,
                    "s3_key": f"{object_prefix}{os.path.basename(file_path)}",
                    "status": "error",
                    "error": str(e)
                }
                uploaded_files.append(error_info)
                self.logger.error(f"Failed to upload {file_path} to S3: {e}")
                
        return uploaded_files

    def _verify_s3_connectivity(self, bucket_name: str) -> bool:
        """
        Verify S3 connectivity and bucket access.
        
        Args:
            bucket_name: S3 bucket name to verify
            
        Returns:
            True if accessible, False otherwise
        """
        try:
            # Try to list objects in the bucket (with limit to avoid large responses)
            self.s3_client.list_objects_v2(Bucket=bucket_name, MaxKeys=1)
            return True
        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == 'NoSuchBucket':
                self.logger.error(f"S3 bucket does not exist: {bucket_name}")
            elif error_code == 'AccessDenied':
                self.logger.error(f"Access denied to S3 bucket: {bucket_name}")
            else:
                self.logger.error(f"S3 error: {error_code} - {e.response['Error']['Message']}")
            return False
        except Exception as e:
            self.logger.error(f"Unexpected error verifying S3 connectivity: {e}")
            return False

    def teardown(self, context: PipelineContext) -> None:
        """Cleanup after execution."""
        # Close S3 client if needed (boto3 clients are usually auto-managed)
        self.s3_client = None
        self.logger.debug("S3 client cleanup completed")